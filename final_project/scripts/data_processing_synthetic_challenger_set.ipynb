{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Groundedness Dataset Generator for Google Colab"
      ],
      "metadata": {
        "id": "WhEceUh2V1ny"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmabpOqCUZit"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install dependencies\n",
        "#!pip install openai requests beautifulsoup4 pandas\n",
        "\n",
        "# Step 2: Import libraries\n",
        "from openai import OpenAI\n",
        "import requests\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "\n",
        "# Step 3: CONFIGURATION\n",
        "OpenAI.api_key = \"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\"\n",
        "\n",
        "client = OpenAI(api_key=OpenAI.api_key)\n",
        "\n",
        "NUM_EXAMPLES = 5\n",
        "MODEL_GROUNDED = \"gpt-4\"         # Options: \"gpt-4\" or \"gpt-3.5-turbo\"\n",
        "MODEL_HALLUCINATED = \"gpt-3.5-turbo\"  # Use GPT-3.5 to reduce cost\n",
        "SAVE_PATH = \"wellsfargo_groundedness_test.csv\"\n",
        "\n",
        "# Step 3: Set up Wells Fargo retail banking webpages:\n",
        "URLS = [\n",
        "    \"https://www.wellsfargo.com/checking/\",\n",
        "    \"https://www.wellsfargo.com/savings-cds/\",\n",
        "    \"https://www.wellsfargo.com/credit-cards/\",\n",
        "    \"https://www.wellsfargo.com/personal-loans/\",\n",
        "    \"https://www.wellsfargo.com/mortgage/\",\n",
        "    \"https://www.wellsfargo.com/debit-card/\",\n",
        "    \"https://www.wellsfargo.com/online-banking/\",\n",
        "    \"https://www.wellsfargo.com/auto-loans/\",\n",
        "    \"https://www.wellsfargo.com/foreign-exchange/\",\n",
        "    \"https://www.wellsfargo.com/help/credit-cards/\",\n",
        "]\n",
        "\n",
        "def check_urls(url_list):\n",
        "    results = []\n",
        "    for url in url_list:\n",
        "        try:\n",
        "            response = requests.head(url, allow_redirects=True, timeout=10)\n",
        "            results.append((url, response.status_code))\n",
        "        except Exception as e:\n",
        "            results.append((url, f\"ERROR: {e}\"))\n",
        "    return results\n",
        "\n",
        "results = check_urls(URLS)\n",
        "\n",
        "# Print non-200 results\n",
        "for url, status in results:\n",
        "    if status != 200:\n",
        "        print(f\"{status} --> {url}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E_H16nAMVy3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Scraper\n",
        "\n",
        "def extract_visible_text(url):\n",
        "    print(f\"Scraping: {url}\")\n",
        "    try:\n",
        "        res = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "        # Remove non-visible and decorative elements\n",
        "        for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\", \"noscript\", \"iframe\"]):\n",
        "            tag.decompose()\n",
        "        for element in soup(text=lambda text: isinstance(text, Comment)):\n",
        "            element.extract()\n",
        "\n",
        "        text = \" \".join(soup.stripped_strings)\n",
        "        return text[:4000]\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Step 7: Generate page-aligned query\n",
        "\n",
        "def generate_query_from_context(context):\n",
        "    prompt = f\"\"\"You are a customer of a bank reading the information below. Write a realistic question you might ask based on the content.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Query:\"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.5,\n",
        "            max_tokens=100,\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"OpenAI API error: {e}\")\n",
        "        return \"ERROR\"\n",
        "\n",
        "# Step 8: Response generation\n",
        "\n",
        "def generate_response(context, query, grounded=True, model=\"gpt-4\"):\n",
        "    if grounded:\n",
        "        prompt = f\"\"\"You are a helpful banking assistant. Use only the context below to answer the query.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"You are a banking assistant. Answer the query, but introduce a subtle inaccuracy or hallucination that is not supported by the context.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=model,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.7 if not grounded else 0.2,\n",
        "            max_tokens=300,\n",
        "        )\n",
        "        return prompt, response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"OpenAI API error: {e}\")\n",
        "        return prompt, \"ERROR\"\n",
        "\n",
        "# Step 9: Dataset generation\n",
        "\n",
        "all_data = []\n",
        "\n",
        "for url in URLS:\n",
        "    context = extract_visible_text(url)\n",
        "    if not context:\n",
        "        continue\n",
        "\n",
        "    query = generate_query_from_context(context)\n",
        "    if query == \"ERROR\":\n",
        "        continue\n",
        "\n",
        "    g_prompt, grounded = generate_response(context, query, grounded=True, model=MODEL_GROUNDED)\n",
        "    h_prompt, hallucinated = generate_response(context, query, grounded=False, model=MODEL_HALLUCINATED)\n",
        "\n",
        "    if \"ERROR\" not in (grounded, hallucinated):\n",
        "        all_data.append({\"url\": url, \"query\": query, \"context\": context, \"prompt\": g_prompt, \"response\": grounded, \"Grounded\": \"PASS\"})\n",
        "        all_data.append({\"url\": url, \"query\": query, \"context\": context, \"prompt\": h_prompt, \"response\": hallucinated, \"Grounded\": \"FAIL\"})\n",
        "        print(f\"✓ Generated: {query}\")\n",
        "    else:\n",
        "        print(\"✗ Skipped due to API error\")\n",
        "    time.sleep(1.5)\n",
        "\n",
        "# Step 10: Save to CSV\n",
        "pd.DataFrame(all_data).to_csv(SAVE_PATH, index=False)\n",
        "print(f\"✅ Dataset saved to {SAVE_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iB-owoHqc1t",
        "outputId": "040a3cd5-fa86-42f3-fcfc-deb22dc046a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: https://www.wellsfargo.com/checking/\n",
            "✓ Generated: What are the specific benefits and features of the Prime Checking and Premier Checking accounts compared to the Everyday Checking and Clear Access Banking options?\n",
            "Scraping: https://www.wellsfargo.com/savings-cds/\n",
            "✓ Generated: What is the process for opening a joint savings account or CD online with Wells Fargo?\n",
            "Scraping: https://www.wellsfargo.com/credit-cards/\n",
            "✓ Generated: What are the specific terms and conditions for earning and redeeming the points on the Autograph Journey service mark ℠ Card?\n",
            "Scraping: https://www.wellsfargo.com/personal-loans/\n",
            "✓ Generated: What is the maximum loan amount I can qualify for with a Wells Fargo personal loan?\n",
            "Scraping: https://www.wellsfargo.com/mortgage/\n",
            "✓ Generated: What are the eligibility requirements for the Homebuyer Access℠ grant and Dream. Plan. Home. ® closing cost credit mentioned in the information?\n",
            "Scraping: https://www.wellsfargo.com/debit-card/\n",
            "✓ Generated: Can you provide more information on the fees associated with using my Wells Fargo Debit Card at non-Wells Fargo ATMs, both domestically and internationally?\n",
            "Scraping: https://www.wellsfargo.com/online-banking/\n",
            "✓ Generated: Is there a fee for using Zelle® through Wells Fargo's mobile and online banking services?\n",
            "Scraping: https://www.wellsfargo.com/auto-loans/\n",
            "✓ Generated: What are the benefits of financing an electric vehicle through Wells Fargo compared to other lenders?\n",
            "Scraping: https://www.wellsfargo.com/foreign-exchange/\n",
            "✓ Generated: What is the process for ordering foreign currency cash online as a Wells Fargo account holder?\n",
            "Scraping: https://www.wellsfargo.com/help/credit-cards/\n",
            "✓ Generated: What steps should I take if I suspect unauthorized transactions on my credit card after reporting it as lost or stolen?\n",
            "✅ Dataset saved to wellsfargo_groundedness_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "URLS = [\n",
        "    \"https://www.wellsfargo.com/checking/\",\n",
        "    \"https://www.wellsfargo.com/savings-cds/\",\n",
        "    \"https://www.wellsfargo.com/credit-cards/\",\n",
        "    \"https://www.wellsfargo.com/personal-loans/\",\n",
        "    \"https://www.wellsfargo.com/mortgage/\",\n",
        "    \"https://www.wellsfargo.com/debit-card/\",\n",
        "    \"https://www.wellsfargo.com/online-banking/\",\n",
        "    \"https://www.wellsfargo.com/auto-loans/\",\n",
        "    \"https://www.wellsfargo.com/foreign-exchange/\",\n",
        "    \"https://www.wellsfargo.com/checking/everyday/\",\n",
        "    \"https://www.wellsfargo.com/mobile-online-banking/manage-accounts/\",\n",
        "    \"https://www.wellsfargo.com/online-banking/deposit-details/\",\n",
        "    \"https://www.wellsfargo.com/online-banking/consumer-account-fees/\",\n",
        "    \"https://www.wellsfargo.com/help/online-banking/\",\n",
        "    \"https://www.wellsfargo.com/help/online-banking/services/\",\n",
        "    \"https://www.wellsfargo.com/checking/overdraft-services/\",\n",
        "    \"https://www.wellsfargo.com/online-banking/zelle/\",\n",
        "    \"https://www.wellsfargo.com/checking/direct-deposit/\",\n",
        "    \"https://www.wellsfargo.com/locator/\",\n",
        "    \"https://www.wellsfargo.com/privacy-security/fraud/\",\n",
        "    \"https://www.wellsfargo.com/mobile/\",\n",
        "    \"https://www.wellsfargo.com/online-banking/statements/\",\n",
        "    \"https://www.wellsfargo.com/online-banking/alerts/\",\n",
        "    \"https://www.wellsfargo.com/online-banking/transfers/\",\n",
        "    \"https://www.wellsfargo.com/investing/\",\n",
        "    \"https://www.wellsfargo.com/financial-education/\",\n",
        "\n",
        "]\n"
      ],
      "metadata": {
        "id": "ez7cjuK0q0sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/google-deepmind/long-form-factuality/tree/main\n",
        "#https://arxiv.org/pdf/2403.18802\n",
        "\n",
        "\n",
        "!pip install datasets --upgrade\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYMtBl8PscfS",
        "outputId": "8c1382e3-4f1e-414e-c486-25157982f70b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"allenai/longfact\", split=\"test\")\n",
        "dataset[0]\n"
      ],
      "metadata": {
        "id": "qNEZXq2YswTk",
        "outputId": "4330e1ea-dab8-434a-97a0-837ed936f926",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "DatasetNotFoundError",
          "evalue": "Dataset 'allenai/longfact' doesn't exist on the Hub or cannot be accessed.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDatasetNotFoundError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-2cf72caf324f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allenai/longfact\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2062\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2063\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1783\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't reach the Hugging Face Hub for dataset '{path}': {e1}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDataFilesNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmptyDatasetError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1576\u001b[0m                 ) from e\n\u001b[1;32m   1577\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1578\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mDatasetNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1579\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m                 dataset_script_path = api.hf_hub_download(\n",
            "\u001b[0;31mDatasetNotFoundError\u001b[0m: Dataset 'allenai/longfact' doesn't exist on the Hub or cannot be accessed."
          ]
        }
      ]
    }
  ]
}