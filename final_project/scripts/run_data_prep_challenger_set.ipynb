{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe2f02b8",
   "metadata": {},
   "source": [
    "# Synthetic Groundedness Dataset Preparation - Challenger Set of Bank Retail Customer Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4c0419b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if needed)\n",
    "#!pip install openai requests pandas\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "from openai import OpenAI, OpenAIError, RateLimitError\n",
    "import requests\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "# Configure API client\n",
    "OpenAI.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=OpenAI.api_key)\n",
    "\n",
    "# Define which models to use for context generation, grounded responses, and hallucinated responses.\n",
    "MODEL_GENERATE_CONTEXT = \"gpt-4o-mini\" \n",
    "MODEL_GROUNDED = \"gpt-4o\"         \n",
    "MODEL_HALLUCINATED = \"gpt-3.5-turbo-0125\"\n",
    "\n",
    "# File paths for data flow\n",
    "SAMPLED_QUERIES_PATH = \"../data/sampled_banking_77_queries.csv\"\n",
    "CONTEXT_OUTPUT_PATH = \"../data/sampled_banking_77_queries_with_context.csv\"\n",
    "FINAL_OUTPUT_PATH = \"../data/synthetic_groundedness_challenger_set.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e565c102",
   "metadata": {},
   "source": [
    "### Sampling Customer Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f4bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load initial customer query dataset\n",
    "df = pd.read_csv(\"../data/banking_77_train_set.csv\")\n",
    "df = df.dropna(subset=[\"text\"])    # Ensure no missing queries\n",
    "df = df.rename(columns={\"text\": \"query\"})  # Rename 'text' to 'query' for consistency\n",
    "\n",
    "# Randomly sample five customer queries\n",
    "#sampled_random = df.sample(n=5, random_state=42) # small sample for initial testing of endpoint and prompting\n",
    "#sampled_random.to_csv(\"../data/test_customer_queries.csv\", index=False)\n",
    "\n",
    "# Sample evenly across categories\n",
    "n_per_category = 5  \n",
    "df_sampled = (\n",
    "    df.groupby(\"category\", group_keys=False)\n",
    "      .apply(lambda x: x.sample(min(len(x), n_per_category), random_state=42))\n",
    ")\n",
    "\n",
    "# Save sampled queries\n",
    "df_sampled.to_csv(SAMPLED_QUERIES_PATH, index=False)\n",
    "print(f\"✅ Saved {len(df_sampled)} sampled rows across {df_sampled['category'].nunique()} categories to {SAMPLED_QUERIES_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadf4c95",
   "metadata": {},
   "source": [
    "### Generate Synthetic FAQ Context for Each Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50435f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section generates realistic help content using GPT for each sampled query.\n",
    "SAVE_INTERVAL = 40\n",
    "MAX_RETRIES = 6\n",
    "SLEEP_BETWEEN_REQUESTS = 1  # seconds\n",
    "\n",
    "df_sampled = pd.read_csv(SAMPLED_QUERIES_PATH)\n",
    "\n",
    "# Prompt template for generating help content\n",
    "def generate_context_prompt(query, category):\n",
    "    return f\"\"\"You are writing content for the Help or FAQ section of a retail bank's website. \n",
    "\n",
    "Write a synthetic help page excerpt (150–250 words) that would support answering the following customer query, using appropriate terminology and informative tone typical of a real bank website. Can you use similar information as what you would find on the big four banks in Australia's websites and make sure they are banking products you would find in Australia. \n",
    "\n",
    "Category: {category}\n",
    "Query: \"{query}\"\n",
    "\"\"\"\n",
    "\n",
    "# Load existing file or initialize fresh run\n",
    "df_sampled[\"context\"] = None\n",
    "if Path(CONTEXT_OUTPUT_PATH).exists():\n",
    "    df_existing = pd.read_csv(CONTEXT_OUTPUT_PATH)\n",
    "    df_sampled.loc[df_existing.index, \"context\"] = df_existing[\"context\"]\n",
    "    print(f\"Resuming from row {df_existing['context'].last_valid_index() + 1}\")\n",
    "\n",
    "# Generate missing contexts using GPT\n",
    "for i, row in df_sampled.iterrows():\n",
    "    if pd.notnull(row[\"context\"]):\n",
    "        continue\n",
    "\n",
    "    prompt = generate_context_prompt(row[\"query\"], row[\"category\"])\n",
    "    retries = 0\n",
    "    context = None\n",
    "\n",
    "    while retries < MAX_RETRIES:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL_GENERATE_CONTEXT,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7\n",
    "            )\n",
    "            context = response.choices[0].message.content.strip()\n",
    "            break\n",
    "        except RateLimitError:\n",
    "            wait_time = 2 ** retries\n",
    "            print(f\"[Rate Limit] Row {i}: waiting {wait_time}s (retry {retries + 1})\")\n",
    "            time.sleep(wait_time)\n",
    "            retries += 1\n",
    "        except OpenAIError as e:\n",
    "            print(f\"[OpenAIError] Row {i}: {e}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"[Unexpected Error] Row {i}: {e}\")\n",
    "            break\n",
    "\n",
    "    df_sampled.at[i, \"context\"] = context if context else \"ERROR\"\n",
    "    time.sleep(SLEEP_BETWEEN_REQUESTS)\n",
    "\n",
    "    if i % SAVE_INTERVAL == 0 or i == len(df_sampled) - 1:\n",
    "        df_sampled.to_csv(CONTEXT_OUTPUT_PATH, index=False)\n",
    "        print(f\"Progress saved up to row {i}\")\n",
    "\n",
    "print(\"✅ Context generation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ccd872",
   "metadata": {},
   "source": [
    "#### Generate Grounded and Hallucinated Responses, and save final Challenger Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c8e41214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section loads the context-enriched queries to generate grounded and hallucinated responses.\n",
    "# Load previously generated synthetic context data\n",
    "df = pd.read_csv(CONTEXT_OUTPUT_PATH)\n",
    "df[\"response_grounded\"] = None\n",
    "df[\"response_ungrounded\"] = None\n",
    "\n",
    "# Define prompt templates\n",
    "# These functions generate the instructions for GPT to create grounded or hallucinated answers.\n",
    "def prompt_grounded(query, context):\n",
    "    return f\"\"\"You are a customer support assistant. Using ONLY the information provided below, write a helpful and accurate answer to the customer query.\n",
    "\n",
    "DO NOT include any facts, assumptions, or language that are not directly supported by the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Query:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "def prompt_ungrounded(query, context):\n",
    "    return f\"\"\"You are a customer support assistant. Write a plausible-sounding response to the customer's query, but include subtle hallucinations.\n",
    "\n",
    "- Change numbers (e.g., fees, timeframes)\n",
    "- Make up a service/product\n",
    "- Assume a specific type of card or account\n",
    "- Repeat the first part of the query in the response before shifting to unrelated details\n",
    "\n",
    "Do NOT reuse the content directly from context. Make the hallucination subtle but detectable.\n",
    "\n",
    "Query:\n",
    "{query}\n",
    "\n",
    "Context (do NOT copy from this):\n",
    "{context}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c8dc7f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Progress saved at index 0\n",
      "✅ Progress saved at index 20\n",
      "✅ Progress saved at index 40\n",
      "✅ Progress saved at index 60\n",
      "✅ Progress saved at index 80\n",
      "✅ Progress saved at index 100\n",
      "✅ Progress saved at index 120\n",
      "✅ Progress saved at index 140\n",
      "✅ Progress saved at index 160\n",
      "✅ Progress saved at index 180\n",
      "✅ Progress saved at index 200\n",
      "✅ Progress saved at index 220\n",
      "✅ Progress saved at index 240\n",
      "✅ Progress saved at index 260\n",
      "✅ Progress saved at index 280\n",
      "✅ Progress saved at index 300\n",
      "✅ Progress saved at index 320\n",
      "✅ Progress saved at index 340\n",
      "✅ Progress saved at index 360\n",
      "✅ Progress saved at index 380\n",
      "✅ Final data saved to ../data/synthetic_groundedness_challenger_set.csv\n"
     ]
    }
   ],
   "source": [
    "# API Wrapper for Completion Calls\n",
    "# Handles GPT API calls with retry logic in case of errors or rate limits.\n",
    "def generate_response(prompt, model, temperature=0.0, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"Retry {attempt + 1} failed: {e}\")\n",
    "            time.sleep(2)\n",
    "    return None\n",
    "\n",
    "# Generate and Save Responses - generates grounded and hallucinated answers for each query and saves output incrementally.\n",
    "for i, row in df.iterrows():\n",
    "    if pd.notnull(row[\"response_grounded\"]) and pd.notnull(row[\"response_ungrounded\"]):\n",
    "        continue\n",
    "\n",
    "    query = row[\"query\"]\n",
    "    context = row[\"context\"]\n",
    "\n",
    "    grounded = generate_response(prompt_grounded(query, context), MODEL_GROUNDED, temperature=0.0)\n",
    "    time.sleep(1)\n",
    "    ungrounded = generate_response(prompt_ungrounded(query, context), MODEL_HALLUCINATED, temperature=0.8)\n",
    "    time.sleep(1)\n",
    "\n",
    "    df.at[i, \"response_grounded\"] = grounded\n",
    "    df.at[i, \"response_ungrounded\"] = ungrounded\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        df.to_csv(FINAL_OUTPUT_PATH, index=False)\n",
    "        print(f\"✅ Progress saved at index {i}\")\n",
    "\n",
    "# Final save\n",
    "df.to_csv(FINAL_OUTPUT_PATH, index=False)\n",
    "print(f\"✅ Final data saved to {FINAL_OUTPUT_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
