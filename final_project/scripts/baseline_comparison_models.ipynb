{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe2f02b8",
   "metadata": {},
   "source": [
    "# Groundedness Dataset Preparation using Benchmark HaluEval Dataset\n",
    "\n",
    "This notebook generates a groundedness dataset from QA-style JSONL input files obtained from:\n",
    " \n",
    "- https://github.com/RUCAIBox/HaluEval/blob/main/README.md\n",
    "- https://github.com/RUCAIBox/HaluEval/blob/main/data/qa_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e565c102",
   "metadata": {},
   "source": [
    "### Define the LLM Evaluator for Groundedness Guardrail using a prompted GPT-4o model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3cb575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# LLM Groundedness Guardrail\n",
    "def call_llm_guardrail(row,openai_client, template, model=\"gpt-4\"):\n",
    "    prompt = template.render({\n",
    "        \"user_query\": row[\"query\"],\n",
    "        \"retrieved_context\": row[\"context\"],\n",
    "        \"model_answer\": row[\"response\"]\n",
    "    })\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        raw_output = response.choices[0].message.content.strip()\n",
    "        parsed = json.loads(raw_output) if raw_output.startswith(\"{\") else {\n",
    "            \"REASONING\": [raw_output],\n",
    "            \"SCORE\": \"FAIL\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        parsed = {\n",
    "            \"REASONING\": [f\"Error: {str(e)}\"],\n",
    "            \"SCORE\": \"FAIL\"\n",
    "        }\n",
    "\n",
    "    return pd.Series({\n",
    "        \"llm_score\": parsed.get(\"SCORE\", \"FAIL\"),\n",
    "        \"llm_reasoning\": \" | \".join(parsed.get(\"REASONING\", [\"Missing reasoning\"]))\n",
    "    })\n",
    "\n",
    "\n",
    "def run_llm_guardrail_batch(df: pd.DataFrame, openai_client=None, template=None, model=\"gpt-4\") -> pd.DataFrame:\n",
    "    results = df.apply(call_llm_guardrail, axis=1, args=(openai_client, template, model))\n",
    "    return pd.concat([df, results], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553797aa",
   "metadata": {},
   "source": [
    "### Define an Off-the-Shelf Option using an AWS Bedrock Groundedness Guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91683dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Bedrock Guardrail\n",
    "def call_bedrock_guardrail(row, guardrail_id, guardrail_version, bedrock_client):\n",
    "    payload = {\n",
    "        \"source\": \"OUTPUT\",\n",
    "        \"content\": [\n",
    "            {\"text\": {\"text\": row[\"context\"], \"qualifiers\": [\"grounding_source\"]}},\n",
    "            {\"text\": {\"text\": row[\"query\"], \"qualifiers\": [\"query\"]}},\n",
    "            {\"text\": {\"text\": row[\"answer\"]}}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = bedrock_client.apply_guardrail(\n",
    "            guardrailIdentifier=guardrail_id,\n",
    "            guardrailVersion=guardrail_version,\n",
    "            source=payload[\"source\"],\n",
    "            content=payload[\"content\"]\n",
    "        )\n",
    "        outputs = response.get(\"outputs\", [{}])\n",
    "        blocked_output = outputs[0].get(\"text\") if outputs else None\n",
    "\n",
    "        grounding_score = None\n",
    "        threshold = None\n",
    "        reason = None\n",
    "        for a in response.get(\"assessments\", []):\n",
    "            if \"groundingPolicy\" in a:\n",
    "                gp = a[\"groundingPolicy\"]\n",
    "                grounding_score = gp.get(\"score\")\n",
    "                threshold = gp.get(\"threshold\")\n",
    "                reason = gp.get(\"action\")\n",
    "\n",
    "        return pd.Series({\n",
    "            \"bedrock_action\": response.get(\"action\", \"UNKNOWN\"),\n",
    "            \"grounding_score\": grounding_score,\n",
    "            \"grounding_threshold\": threshold,\n",
    "            \"grounding_decision_reason\": reason,\n",
    "            \"blocked_output_text\": blocked_output\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        return pd.Series({\n",
    "            \"bedrock_action\": \"ERROR\",\n",
    "            \"grounding_score\": None,\n",
    "            \"grounding_threshold\": None,\n",
    "            \"grounding_decision_reason\": str(e),\n",
    "            \"blocked_output_text\": None\n",
    "        })\n",
    "\n",
    "\n",
    "def run_bedrock_guardrail_batch(df: pd.DataFrame, guardrail_id: str, guardrail_version: str, bedrock_client) -> pd.DataFrame:\n",
    "    results = df.apply(call_bedrock_guardrail, axis=1, args=(guardrail_id, guardrail_version, bedrock_client))\n",
    "    return pd.concat([df, results], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
