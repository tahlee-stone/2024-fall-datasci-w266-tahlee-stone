{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe2f02b8",
   "metadata": {},
   "source": [
    "# Run Comparison Models - LLM Groundedness Evaluator and Off-shelf Groundedness Guardrail\n",
    "\n",
    "This notebook establishes the required connections and sets up two comparison models - a prompted LLM GPT-4o evaluator and an off-the-shelf AWS Bedrock guardrail. For more information on the AWS Bedrock contextual grounding guardrail please see:\n",
    "\n",
    "- https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-contextual-grounding-check.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "372ea189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import boto3\n",
    "from openai import OpenAI\n",
    "from jinja2 import Environment, FileSystemLoader, select_autoescape\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "839561bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UserId': 'AIDASYIUVQAL4IXIIR4FU', 'Account': '189558784023', 'Arn': 'arn:aws:iam::189558784023:user/TStone', 'ResponseMetadata': {'RequestId': 'd3149f8c-9ce8-4434-9d82-b59b4bcd29ce', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'd3149f8c-9ce8-4434-9d82-b59b4bcd29ce', 'content-type': 'text/xml', 'content-length': '403', 'date': 'Sun, 29 Jun 2025 14:08:11 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Explicitly extract the credentials\n",
    "access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "region = os.getenv(\"AWS_DEFAULT_REGION\")\n",
    "\n",
    "# Pass them to boto3\n",
    "bedrock_client = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=region,\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key\n",
    ")\n",
    "# Pass them to boto3\n",
    "bedrock_client\n",
    "\n",
    "sts_client = boto3.client(\n",
    "    \"sts\",\n",
    "    region_name=region,\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key\n",
    ")\n",
    "\n",
    "print(sts_client.get_caller_identity())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b229655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "# Configure OpenAI and AWS API clients\n",
    "OpenAI.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_client = OpenAI(api_key=OpenAI.api_key)\n",
    "#bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "# === Load Prompt Template ===\n",
    "env = Environment(\n",
    "    loader=FileSystemLoader(\".\"), \n",
    "    autoescape=select_autoescape()\n",
    ")\n",
    "template = env.get_template(\"groundedness_guardrail.j2\")\n",
    "\n",
    "# === Configuration ===\n",
    "GUARDRAIL_ID = \"egi3t9xv4xej\"\n",
    "GUARDRAIL_VERSION = \"1\"\n",
    "MODEL_NAME = \"gpt-4o\"\n",
    "sample_size = 500\n",
    "SAVE_INTERVAL = 40\n",
    "#input_file = \"../data/halueval_groundedness.csv\"\n",
    "#output_file = \"../data/halueval_bedrock_results.csv\"\n",
    "\n",
    "input_file = \"../data/synthetic_groundedness_challenger_set_long.csv\"\n",
    "output_file = \"../data/synthetic_groundedness_challenger_set_bedrock_results.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e565c102",
   "metadata": {},
   "source": [
    "### Define the LLM Evaluator for Groundedness Guardrail using a prompted GPT-4o model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdd583ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Groundedness Guardrail\n",
    "def call_llm_guardrail(row, openai_client, template, model=\"gpt-4\"):\n",
    "    prompt = template.render({\n",
    "        \"user_query\": row[\"query\"],\n",
    "        \"retrieved_context\": row[\"context\"],\n",
    "        \"model_answer\": row[\"response\"]\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        raw_output = response.choices[0].message.content.strip()\n",
    "\n",
    "        # --- Fix: strip markdown fencing if present ---\n",
    "\n",
    "        if raw_output.startswith(\"```\"):\n",
    "            raw_output = raw_output.replace(\"```json\", \"\", 1).replace(\"```\", \"\").strip()\n",
    "\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(raw_output)\n",
    "        except json.JSONDecodeError:\n",
    "            parsed = {\n",
    "                \"REASONING\": [raw_output],\n",
    "                \"SCORE\": \"FAIL\"\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        parsed = {\n",
    "            \"REASONING\": [f\"Error: {str(e)}\"],\n",
    "            \"SCORE\": \"FAIL\"\n",
    "        }\n",
    "\n",
    "    return pd.Series({\n",
    "        \"llm_score\": parsed.get(\"SCORE\", \"FAIL\"),\n",
    "        \"llm_reasoning\": \" | \".join(parsed.get(\"REASONING\", [\"Missing reasoning\"]))\n",
    "    })\n",
    "\n",
    "\n",
    "def run_llm_guardrail_batch_with_resume(input_file, output_file, openai_client, template, model=\"gpt-4\", save_interval=20):\n",
    "    full_dataset = pd.read_csv(input_file)\n",
    "\n",
    "    try:\n",
    "        completed_results = pd.read_csv(output_file)\n",
    "        resume_from_index = len(completed_results)\n",
    "        print(f\"Resuming from row {resume_from_index}\")\n",
    "        if not completed_results.empty:\n",
    "            completed_results = completed_results.astype(full_dataset.dtypes.to_dict(), errors='ignore')\n",
    "    except FileNotFoundError:\n",
    "        completed_results = pd.DataFrame()\n",
    "        resume_from_index = 0\n",
    "        print(\"No existing output found. Starting fresh.\")\n",
    "\n",
    "    remaining_data = full_dataset.iloc[resume_from_index:].copy()\n",
    "\n",
    "    for i, (_, row) in enumerate(remaining_data.iterrows(), start=resume_from_index):\n",
    "        result = call_llm_guardrail(row, openai_client, template, model=model)\n",
    "        completed_row = pd.concat([row, result])\n",
    "        completed_results = pd.concat([completed_results, completed_row.to_frame().T], ignore_index=True)\n",
    "\n",
    "        if (i + 1) % save_interval == 0 or (i + 1) == len(full_dataset):\n",
    "            completed_results.to_csv(output_file, index=False)\n",
    "            print(f\"Saved progress at row {i + 1}\")\n",
    "\n",
    "    return completed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bbe4526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing output found. Starting fresh.\n",
      "Saved progress at row 40\n",
      "Saved progress at row 80\n",
      "Saved progress at row 120\n",
      "Saved progress at row 160\n",
      "Saved progress at row 200\n",
      "Saved progress at row 240\n",
      "Saved progress at row 280\n",
      "Saved progress at row 320\n",
      "Saved progress at row 360\n",
      "Saved progress at row 400\n",
      "Saved progress at row 440\n",
      "Saved progress at row 480\n",
      "Saved progress at row 520\n",
      "Saved progress at row 560\n",
      "Saved progress at row 600\n",
      "Saved progress at row 640\n",
      "Saved progress at row 680\n",
      "Saved progress at row 720\n",
      "Saved progress at row 760\n",
      "Saved progress at row 770\n"
     ]
    }
   ],
   "source": [
    "# === Step 2: Run LLM Guardrail ===\n",
    "df = run_llm_guardrail_batch_with_resume(input_file, output_file, openai_client=openai_client, template=template, model=MODEL_NAME,save_interval = SAVE_INTERVAL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553797aa",
   "metadata": {},
   "source": [
    "### Define an Off-the-Shelf Option using an AWS Bedrock Groundedness Guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "91683dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Bedrock Guardrail\n",
    "def call_bedrock_guardrail(row, guardrail_id, guardrail_version, bedrock_client):\n",
    "    payload = {\n",
    "        \"source\": \"OUTPUT\",\n",
    "        \"content\": [\n",
    "            {\"text\": {\"text\": row[\"context\"], \"qualifiers\": [\"grounding_source\"]}},\n",
    "            {\"text\": {\"text\": row[\"query\"], \"qualifiers\": [\"query\"]}},\n",
    "            {\"text\": {\"text\": row[\"response\"]}}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = bedrock_client.apply_guardrail(\n",
    "            guardrailIdentifier=guardrail_id,\n",
    "            guardrailVersion=guardrail_version,\n",
    "            source=payload[\"source\"],\n",
    "            content=payload[\"content\"]\n",
    "        )\n",
    "        outputs = response.get(\"outputs\", [{}])\n",
    "        blocked_output = outputs[0].get(\"text\") if outputs else None\n",
    "\n",
    "        grounding_score = None\n",
    "        threshold = None\n",
    "        reason = None\n",
    "        for a in response.get(\"assessments\", []):\n",
    "            if \"groundingPolicy\" in a:\n",
    "                gp = a[\"groundingPolicy\"]\n",
    "                grounding_score = gp.get(\"score\")\n",
    "                threshold = gp.get(\"threshold\")\n",
    "                reason = gp.get(\"action\")\n",
    "\n",
    "        return pd.Series({\n",
    "            \"bedrock_action\": response.get(\"action\", \"UNKNOWN\"),\n",
    "            \"grounding_score\": grounding_score,\n",
    "            \"grounding_threshold\": threshold,\n",
    "            \"grounding_decision_reason\": reason,\n",
    "            \"blocked_output_text\": blocked_output\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        return pd.Series({\n",
    "            \"bedrock_action\": \"ERROR\",\n",
    "            \"grounding_score\": None,\n",
    "            \"grounding_threshold\": None,\n",
    "            \"grounding_decision_reason\": str(e),\n",
    "            \"blocked_output_text\": None\n",
    "        })\n",
    "\n",
    "\n",
    "def run_bedrock_guardrail_batch(df: pd.DataFrame, guardrail_id: str, guardrail_version: str, bedrock_client) -> pd.DataFrame:\n",
    "    results = df.apply(call_bedrock_guardrail, axis=1, args=(guardrail_id, guardrail_version, bedrock_client))\n",
    "    return pd.concat([df, results], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fc791609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Incremental Batch Processor ===\n",
    "def run_bedrock_guardrail_batch(input_file, output_file, guardrail_id, guardrail_version, bedrock_client, save_interval=20):\n",
    "    full_dataset = pd.read_csv(input_file)\n",
    "\n",
    "    try:\n",
    "        completed_results = pd.read_csv(output_file)\n",
    "        resume_from_index = len(completed_results)\n",
    "        print(f\"Resuming from row {resume_from_index}\")\n",
    "        if not completed_results.empty:\n",
    "            completed_results = completed_results.astype(full_dataset.dtypes.to_dict(), errors=\"ignore\")\n",
    "    except FileNotFoundError:\n",
    "        completed_results = pd.DataFrame()\n",
    "        resume_from_index = 0\n",
    "        print(\"No existing output found. Starting fresh.\")\n",
    "\n",
    "    remaining_data = full_dataset.iloc[resume_from_index:].copy()\n",
    "\n",
    "    for i, (_, row) in enumerate(remaining_data.iterrows(), start=resume_from_index):\n",
    "        result = call_bedrock_guardrail(row, guardrail_id, guardrail_version, bedrock_client)\n",
    "        combined_row = pd.concat([row, result])\n",
    "        completed_results = pd.concat([completed_results, combined_row.to_frame().T], ignore_index=True)\n",
    "\n",
    "        if (i + 1) % save_interval == 0 or (i + 1) == len(full_dataset):\n",
    "            completed_results.to_csv(output_file, index=False)\n",
    "            print(f\"‚úÖ Saved progress at row {i + 1} / {len(full_dataset)}\")\n",
    "\n",
    "    return completed_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "deec5a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing output found. Starting fresh.\n",
      "‚úÖ Saved progress at row 40 / 770\n",
      "‚úÖ Saved progress at row 80 / 770\n",
      "‚úÖ Saved progress at row 120 / 770\n",
      "‚úÖ Saved progress at row 160 / 770\n",
      "‚úÖ Saved progress at row 200 / 770\n",
      "‚úÖ Saved progress at row 240 / 770\n",
      "‚úÖ Saved progress at row 280 / 770\n",
      "‚úÖ Saved progress at row 320 / 770\n",
      "‚úÖ Saved progress at row 360 / 770\n",
      "‚úÖ Saved progress at row 400 / 770\n",
      "‚úÖ Saved progress at row 440 / 770\n",
      "‚úÖ Saved progress at row 480 / 770\n",
      "‚úÖ Saved progress at row 520 / 770\n",
      "‚úÖ Saved progress at row 560 / 770\n",
      "‚úÖ Saved progress at row 600 / 770\n",
      "‚úÖ Saved progress at row 640 / 770\n",
      "‚úÖ Saved progress at row 680 / 770\n",
      "‚úÖ Saved progress at row 720 / 770\n",
      "‚úÖ Saved progress at row 760 / 770\n",
      "‚úÖ Saved progress at row 770 / 770\n"
     ]
    }
   ],
   "source": [
    "# === Step 3: Run Bedrock Guardrail ===\n",
    "df_br = run_bedrock_guardrail_batch(input_file, output_file, guardrail_id=GUARDRAIL_ID, guardrail_version=GUARDRAIL_VERSION, bedrock_client=bedrock_client, save_interval=SAVE_INTERVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17a3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 5: Prediction and Evaluation  ===\n",
    "#df[\"llm_guardrail_pred\"] = df[\"llm_score\"].map(lambda x: 1 if str(x).upper() == \"PASS\" else 0)\n",
    "#df[\"bedrock_pred\"] = df[\"grounding_score\"].apply(lambda x: 1 if x is not None and x >= 0.5 else 0)\n",
    "#df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "#print(\"\\nüìä Model Evaluation Summary:\")\n",
    "#for model_name in [\"llm_guardrail_pred\", \"bedrock_pred\"]:\n",
    "#    print(f\"\\nüîç {model_name}:\\n\")\n",
    "#    print(classification_report(df[\"label\"], df[model_name], target_names=[\"FAIL\", \"PASS\"]))\n",
    "\n",
    "# === Step 6: Save Results ===\n",
    "results_path = \"results/llm_bedrock_evaluation.csv\"\n",
    "df.to_csv(results_path, index=False)\n",
    "print(f\"\\n‚úÖ Evaluation complete. Results saved to '{results_path}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
