{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe2f02b8",
   "metadata": {},
   "source": [
    "# Groundedness Dataset Preparation using Benchmark HaluEval Dataset\n",
    "\n",
    "This notebook generates a groundedness dataset from QA-style JSONL input files obtained from:\n",
    " \n",
    "- https://github.com/RUCAIBox/HaluEval/blob/main/README.md\n",
    "- https://github.com/RUCAIBox/HaluEval/blob/main/data/qa_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ea189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "import boto3\n",
    "from openai import OpenAI\n",
    "from jinja2 import Environment, FileSystemLoader, select_autoescape\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b229655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add src to path if running from project root\n",
    "sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "# === Load environment variables ===\n",
    "load_dotenv()\n",
    "\n",
    "# === AWS + OpenAI clients ===\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "# === Load Prompt Template ===\n",
    "env = Environment(\n",
    "    loader=FileSystemLoader(\"src\"), \n",
    "    autoescape=select_autoescape()\n",
    ")\n",
    "template = env.get_template(\"groundedness_guardrail.j2\")\n",
    "\n",
    "# === Configuration ===\n",
    "GUARDRAIL_ID = \"egi3t9xv4xej\"\n",
    "GUARDRAIL_VERSION = \"1\"\n",
    "MODEL_NAME = \"gpt-4\"\n",
    "sample_size = 500\n",
    "input_json = \"inputs/qa_data.json\"\n",
    "output_csv = \"inputs/halueval_groundedness.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e565c102",
   "metadata": {},
   "source": [
    "### Define the LLM Evaluator for Groundedness Guardrail using a prompted GPT-4o model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af5cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LLM Groundedness Guardrail\n",
    "def call_llm_guardrail(row,openai_client, template, model=\"gpt-4\"):\n",
    "    prompt = template.render({\n",
    "        \"user_query\": row[\"query\"],\n",
    "        \"retrieved_context\": row[\"context\"],\n",
    "        \"model_answer\": row[\"response\"]\n",
    "    })\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0\n",
    "        )\n",
    "        raw_output = response.choices[0].message.content.strip()\n",
    "        parsed = json.loads(raw_output) if raw_output.startswith(\"{\") else {\n",
    "            \"REASONING\": [raw_output],\n",
    "            \"SCORE\": \"FAIL\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        parsed = {\n",
    "            \"REASONING\": [f\"Error: {str(e)}\"],\n",
    "            \"SCORE\": \"FAIL\"\n",
    "        }\n",
    "\n",
    "    return pd.Series({\n",
    "        \"llm_score\": parsed.get(\"SCORE\", \"FAIL\"),\n",
    "        \"llm_reasoning\": \" | \".join(parsed.get(\"REASONING\", [\"Missing reasoning\"]))\n",
    "    })\n",
    "\n",
    "\n",
    "def run_llm_guardrail_batch(df: pd.DataFrame, openai_client=None, template=None, model=\"gpt-4\") -> pd.DataFrame:\n",
    "    results = df.apply(call_llm_guardrail, axis=1, args=(openai_client, template, model))\n",
    "    return pd.concat([df, results], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553797aa",
   "metadata": {},
   "source": [
    "### Define an Off-the-Shelf Option using an AWS Bedrock Groundedness Guardrail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91683dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Bedrock Guardrail\n",
    "def call_bedrock_guardrail(row, guardrail_id, guardrail_version, bedrock_client):\n",
    "    payload = {\n",
    "        \"source\": \"OUTPUT\",\n",
    "        \"content\": [\n",
    "            {\"text\": {\"text\": row[\"context\"], \"qualifiers\": [\"grounding_source\"]}},\n",
    "            {\"text\": {\"text\": row[\"query\"], \"qualifiers\": [\"query\"]}},\n",
    "            {\"text\": {\"text\": row[\"answer\"]}}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = bedrock_client.apply_guardrail(\n",
    "            guardrailIdentifier=guardrail_id,\n",
    "            guardrailVersion=guardrail_version,\n",
    "            source=payload[\"source\"],\n",
    "            content=payload[\"content\"]\n",
    "        )\n",
    "        outputs = response.get(\"outputs\", [{}])\n",
    "        blocked_output = outputs[0].get(\"text\") if outputs else None\n",
    "\n",
    "        grounding_score = None\n",
    "        threshold = None\n",
    "        reason = None\n",
    "        for a in response.get(\"assessments\", []):\n",
    "            if \"groundingPolicy\" in a:\n",
    "                gp = a[\"groundingPolicy\"]\n",
    "                grounding_score = gp.get(\"score\")\n",
    "                threshold = gp.get(\"threshold\")\n",
    "                reason = gp.get(\"action\")\n",
    "\n",
    "        return pd.Series({\n",
    "            \"bedrock_action\": response.get(\"action\", \"UNKNOWN\"),\n",
    "            \"grounding_score\": grounding_score,\n",
    "            \"grounding_threshold\": threshold,\n",
    "            \"grounding_decision_reason\": reason,\n",
    "            \"blocked_output_text\": blocked_output\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        return pd.Series({\n",
    "            \"bedrock_action\": \"ERROR\",\n",
    "            \"grounding_score\": None,\n",
    "            \"grounding_threshold\": None,\n",
    "            \"grounding_decision_reason\": str(e),\n",
    "            \"blocked_output_text\": None\n",
    "        })\n",
    "\n",
    "\n",
    "def run_bedrock_guardrail_batch(df: pd.DataFrame, guardrail_id: str, guardrail_version: str, bedrock_client) -> pd.DataFrame:\n",
    "    results = df.apply(call_bedrock_guardrail, axis=1, args=(guardrail_id, guardrail_version, bedrock_client))\n",
    "    return pd.concat([df, results], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec5a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 1: Load Dataset ===\n",
    "df = pd.read_csv(output_csv)\n",
    "\n",
    "# === Step 2: Run LLM Guardrail ===\n",
    "df = run_llm_guardrail_batch(df.copy(), model=MODEL_NAME, openai_client=openai_client, template=template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b687413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 3: Run Bedrock Guardrail ===\n",
    "df = run_bedrock_guardrail_batch(df, guardrail_id=GUARDRAIL_ID, guardrail_version=GUARDRAIL_VERSION, bedrock_client=bedrock_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17a3b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 5: Prediction and Evaluation  ===\n",
    "#df[\"llm_guardrail_pred\"] = df[\"llm_score\"].map(lambda x: 1 if str(x).upper() == \"PASS\" else 0)\n",
    "#df[\"bedrock_pred\"] = df[\"grounding_score\"].apply(lambda x: 1 if x is not None and x >= 0.5 else 0)\n",
    "#df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "#print(\"\\nüìä Model Evaluation Summary:\")\n",
    "#for model_name in [\"llm_guardrail_pred\", \"bedrock_pred\"]:\n",
    "#    print(f\"\\nüîç {model_name}:\\n\")\n",
    "#    print(classification_report(df[\"label\"], df[model_name], target_names=[\"FAIL\", \"PASS\"]))\n",
    "\n",
    "# === Step 6: Save Results ===\n",
    "results_path = \"results/llm_bedrock_evaluation.csv\"\n",
    "df.to_csv(results_path, index=False)\n",
    "print(f\"\\n‚úÖ Evaluation complete. Results saved to '{results_path}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
